#!/usr/bin/env python3
"""
arXiv Paper Fetcher Tool
æ¯å¤©è‡ªåŠ¨ä» arXiv è·å–æ›´æ–°çš„è®ºæ–‡ï¼Œå¹¶ç­›é€‰å‡ºåŒ…å«ç‰¹å®šå…³é”®è¯çš„è®ºæ–‡
"""

import arxiv
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Set
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
import sys

# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆWindows å…¼å®¹ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('arxiv_fetcher.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ArxivPaperFetcher:
    """arXiv è®ºæ–‡æŠ“å–å’Œç­›é€‰å·¥å…·"""
    
    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–æŠ“å–å·¥å…·
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å¸¦æ—¥æœŸçš„ç›®å½•å
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®å½•ï¼Œä½¿ç”¨å¸¦æ—¥æœŸçš„ç›®å½•å
        if data_dir is None:
            date_str = datetime.now().strftime("%Y.%m.%d")
            data_dir = f"paper_data_{date_str}"
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # å…³é”®è¯åˆ—è¡¨ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        # æ³¨æ„ï¼šVideo Generation å’Œ LLM Training éœ€è¦é¢å¤–æ£€æŸ¥ system ç›¸å…³å…³é”®è¯
        self.keywords = [
            "KV cache",
            "KV Cache",
            "kv cache",
            "KVCache",
            "LLM inference",
            "llm inference",
            "large language model inference",
            "LLM training",
            "llm training",
            "large language model training",
            "LLM communication",
            "llm communication",
            "communication optimization",
            "communication efficient",
            "allreduce",
            "all-gather",
            "collective communication",
            "video generation",
            "video synthesis",
            "video generation model"  # Video Generation éœ€è¦ system ç›¸å…³
        ]
        
        # System ç›¸å…³å…³é”®è¯ï¼ˆç”¨äº RL å’Œ Video Generation çš„é¢å¤–ç­›é€‰ï¼‰
        self.system_keywords = [
            "system",
            "systems",
            "architecture",
            "framework",
            "platform",
            "infrastructure",
            "deployment",
            "serving",
            "serving system",
            "inference system",
            "training system",
            "runtime",
            "engine",
            "pipeline"
        ]
        
        # å·²è®°å½•çš„è®ºæ–‡IDé›†åˆï¼ˆç”¨äºå»é‡ï¼‰
        self.recorded_papers_file = self.data_dir / "recorded_papers.json"
        self.recorded_paper_ids = self._load_recorded_papers()
    
    def _load_recorded_papers(self) -> Set[str]:
        """åŠ è½½å·²è®°å½•çš„è®ºæ–‡ID"""
        if self.recorded_papers_file.exists():
            try:
                with open(self.recorded_papers_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('paper_ids', []))
            except Exception as e:
                logger.warning(f"åŠ è½½å·²è®°å½•è®ºæ–‡å¤±è´¥: {e}")
                return set()
        return set()
    
    def _save_recorded_papers(self):
        """ä¿å­˜å·²è®°å½•çš„è®ºæ–‡ID"""
        try:
            data = {
                'paper_ids': list(self.recorded_paper_ids),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.recorded_papers_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜å·²è®°å½•è®ºæ–‡å¤±è´¥: {e}")
    
    def _check_keywords(self, paper: arxiv.Result) -> bool:
        """
        æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ…å«å…³é”®è¯
        
        Args:
            paper: arxiv è®ºæ–‡å¯¹è±¡
            
        Returns:
            å¦‚æœåŒ…å«å…³é”®è¯è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        import re
        # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦
        text_to_check = f"{paper.title} {paper.summary}".lower()
        
        # æ£€æŸ¥ KV Cache å’Œ LLM Inferenceï¼ˆä¸éœ€è¦ system é™åˆ¶ï¼‰
        if any(kw in text_to_check for kw in ["kv cache", "kvcache"]):
            return True
        
        if any(kw in text_to_check for kw in ["llm inference", "large language model inference"]):
            return True
        
        # LLM Training å¿…é¡»åŒ…å« system ç›¸å…³å…³é”®è¯
        training_keywords = ["llm training", "large language model training"]
        if any(train_kw in text_to_check for train_kw in training_keywords):
            if any(sys_kw in text_to_check for sys_kw in self.system_keywords):
                return True
        
        # LLM Communication ä¼˜åŒ–ç›¸å…³
        comm_keywords = [
            "llm communication",
            "communication optimization",
            "communication efficient",
            "allreduce",
            "all-gather",
            "collective communication",
            "gradient communication",
            "communication compression"
        ]
        if any(comm_kw in text_to_check for comm_kw in comm_keywords):
            return True
        
        # Video Generation å¿…é¡»åŒ…å« system ç›¸å…³å…³é”®è¯
        video_keywords = ["video generation", "video synthesis", "video generation model"]
        if any(video_kw in text_to_check for video_kw in video_keywords):
            # æ£€æŸ¥æ˜¯å¦åŒ…å« system ç›¸å…³å…³é”®è¯
            if any(sys_kw in text_to_check for sys_kw in self.system_keywords):
                return True
        
        return False
    
    def _categorize_paper(self, paper: arxiv.Result) -> List[str]:
        """
        å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»
        
        Args:
            paper: arxiv è®ºæ–‡å¯¹è±¡
            
        Returns:
            åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
        """
        categories = []
        text = f"{paper.title} {paper.summary}".lower()
        
        if any(kw in text for kw in ["kv cache", "kvcache"]):
            categories.append("KV Cache")
        
        if any(kw in text for kw in ["llm inference", "large language model inference"]):
            categories.append("LLM Inference")
        
        # LLM Training å¿…é¡»åŒ…å« system ç›¸å…³å…³é”®è¯
        training_keywords = ["llm training", "large language model training"]
        if any(train_kw in text for train_kw in training_keywords):
            if any(sys_kw in text for sys_kw in self.system_keywords):
                categories.append("LLM Training (System)")
        
        # LLM Communication ä¼˜åŒ–ç›¸å…³
        comm_keywords = [
            "llm communication",
            "communication optimization",
            "communication efficient",
            "allreduce",
            "all-gather",
            "collective communication",
            "gradient communication",
            "communication compression"
        ]
        if any(comm_kw in text for comm_kw in comm_keywords):
            categories.append("LLM Communication")
        
        # Video Generation å¿…é¡»åŒ…å« system ç›¸å…³å…³é”®è¯
        video_keywords = ["video generation", "video synthesis", "video generation model"]
        if any(video_kw in text for video_kw in video_keywords):
            if any(sys_kw in text for sys_kw in self.system_keywords):
                categories.append("Video Generation (System)")
        
        return categories if categories else ["Other"]
    
    def fetch_daily_papers(self, days_back: int = 1, max_results: int = 1000) -> List[Dict]:
        """
        è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
        
        Args:
            days_back: å›æº¯å¤©æ•°ï¼Œé»˜è®¤1å¤©ï¼ˆä»Šå¤©ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹è·å–æœ€è¿‘ {days_back} å¤©çš„ arXiv è®ºæ–‡...")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # æ„å»ºæŸ¥è¯¢ï¼šè·å–æœ€è¿‘æ›´æ–°çš„è®ºæ–‡
        # arXiv ä½¿ç”¨æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD
        date_str = start_date.strftime("%Y%m%d")
        query = f"submittedDate:[{date_str}000000 TO {end_date.strftime('%Y%m%d')}235959]"
        
        logger.info(f"æŸ¥è¯¢æ¡ä»¶: {query}")
        
        # æœç´¢è®ºæ–‡
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        matched_papers = []
        total_checked = 0
        
        try:
            for paper in arxiv.Client().results(search):
                total_checked += 1
                
                # è·³è¿‡å·²è®°å½•çš„è®ºæ–‡
                if paper.entry_id in self.recorded_paper_ids:
                    continue
                
                # æ£€æŸ¥å…³é”®è¯
                if self._check_keywords(paper):
                    categories = self._categorize_paper(paper)
                    paper_info = {
                        'id': paper.entry_id,
                        'arxiv_id': paper.entry_id.split('/')[-1],
                        'title': paper.title,
                        'authors': [author.name for author in paper.authors],
                        'summary': paper.summary,
                        'published': paper.published.isoformat(),
                        'updated': paper.updated.isoformat(),
                        'categories': paper.categories,
                        'tags': categories,
                        'pdf_url': paper.pdf_url,
                        'arxiv_url': paper.entry_id,
                        'found_date': datetime.now().isoformat()
                    }
                    matched_papers.append(paper_info)
                    self.recorded_paper_ids.add(paper.entry_id)
                    
                    logger.info(f"æ‰¾åˆ°åŒ¹é…è®ºæ–‡: {paper.title[:60]}...")
                    logger.info(f"  åˆ†ç±»: {', '.join(categories)}")
                    logger.info(f"  arXiv ID: {paper.entry_id.split('/')[-1]}")
        
        except Exception as e:
            logger.error(f"è·å–è®ºæ–‡æ—¶å‡ºé”™: {e}")
            raise
        
        logger.info(f"å…±æ£€æŸ¥ {total_checked} ç¯‡è®ºæ–‡ï¼Œæ‰¾åˆ° {len(matched_papers)} ç¯‡åŒ¹é…è®ºæ–‡")
        
        return matched_papers
    
    def save_papers(self, papers: List[Dict], filename: str = None):
        """
        ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°æ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨æ—¥æœŸå‘½å
        """
        if not papers:
            logger.info("æ²¡æœ‰æ–°è®ºæ–‡éœ€è¦ä¿å­˜")
            return
        
        if filename is None:
            date_str = datetime.now().strftime("%Y%m%d")
            filename = f"papers_{date_str}.json"
        
        filepath = self.data_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        existing_papers = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_papers = existing_data.get('papers', [])
            except Exception as e:
                logger.warning(f"è¯»å–å·²æœ‰æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆå¹¶å¹¶å»é‡
        all_papers = existing_papers + papers
        seen_ids = set()
        unique_papers = []
        for paper in all_papers:
            if paper['id'] not in seen_ids:
                seen_ids.add(paper['id'])
                unique_papers.append(paper)
        
        # ä¿å­˜
        data = {
            'fetch_date': datetime.now().isoformat(),
            'total_papers': len(unique_papers),
            'papers': unique_papers
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"å·²ä¿å­˜ {len(unique_papers)} ç¯‡è®ºæ–‡åˆ° {filepath}")
        except Exception as e:
            logger.error(f"ä¿å­˜è®ºæ–‡å¤±è´¥: {e}")
            raise
        
        # æ›´æ–°å·²è®°å½•è®ºæ–‡åˆ—è¡¨
        self._save_recorded_papers()
    
    def generate_markdown_report(self, papers: List[Dict], output_file: str = None):
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Šï¼Œä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            output_file: æ€»è§ˆæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        if not papers:
            logger.info("æ²¡æœ‰è®ºæ–‡éœ€è¦ç”ŸæˆæŠ¥å‘Š")
            return
        
        date_str = datetime.now().strftime("%Y%m%d")
        
        # æŒ‰åˆ†ç±»ç»„ç»‡è®ºæ–‡
        papers_by_category = {}
        for paper in papers:
            tags = paper.get('tags', ['Other'])
            for tag in tags:
                if tag not in papers_by_category:
                    papers_by_category[tag] = []
                papers_by_category[tag].append(paper)
        
        # å®šä¹‰åˆ†ç±»é¡ºåº
        category_order = [
            "KV Cache",
            "LLM Inference",
            "LLM Training (System)",
            "LLM Communication",
            "Video Generation (System)"
        ]
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶
        generated_files = []
        for category in category_order:
            if category in papers_by_category:
                category_papers = papers_by_category[category]
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¸åŠ æ—¥æœŸï¼‰
                safe_category_name = category.replace(" ", "_").replace("(", "").replace(")", "").replace("/", "_")
                category_file = f"{safe_category_name}.md"
                category_filepath = self.data_dir / category_file
                
                # ç”Ÿæˆè¯¥åˆ†ç±»çš„ Markdown å†…å®¹
                md_content = f"""# {category} - arXiv è®ºæ–‡æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**åˆ†ç±»**: {category}

**è®ºæ–‡æ•°é‡**: {len(category_papers)} ç¯‡

---

"""
                
                for idx, paper in enumerate(category_papers, 1):
                    authors_str = ", ".join(paper['authors'][:5])
                    if len(paper['authors']) > 5:
                        authors_str += f" et al. ({len(paper['authors'])} authors)"
                    
                    # æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾
                    all_tags = paper.get('tags', [])
                    tags_display = ', '.join(all_tags) if all_tags else 'Other'
                    
                    md_content += f"""## {idx}. {paper['title']}

- **arXiv ID**: [{paper['arxiv_id']}]({paper['arxiv_url']})
- **ä½œè€…**: {authors_str}
- **å‘å¸ƒæ—¶é—´**: {paper['published']}
- **arXivåˆ†ç±»**: {', '.join(paper['categories'])}
- **æ ‡ç­¾**: {tags_display}
- **PDF**: [ä¸‹è½½é“¾æ¥]({paper['pdf_url']})

**æ‘˜è¦**:
{paper['summary']}

---

"""
                
                try:
                    with open(category_filepath, 'w', encoding='utf-8') as f:
                        f.write(md_content)
                    logger.info(f"å·²ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š: {category_filepath}")
                    generated_files.append((category, category_file))
                except Exception as e:
                    logger.error(f"ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šå¤±è´¥ ({category}): {e}")
        
        # ç”Ÿæˆæ€»è§ˆæ–‡ä»¶
        if output_file is None:
            output_file = "arxiv_report.md"
        
        overview_filepath = self.data_dir / output_file
        
        # ç”Ÿæˆæ€»è§ˆå†…å®¹
        category_summary = []
        for category in category_order:
            if category in papers_by_category:
                count = len(papers_by_category[category])
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆä¸åŠ æ—¥æœŸï¼‰
                safe_category_name = category.replace(" ", "_").replace("(", "").replace(")", "").replace("/", "_")
                category_file = f"{safe_category_name}.md"
                category_summary.append(f"- **[{category}]({category_file})**: {count} ç¯‡")
        
        overview_content = f"""# arXiv è®ºæ–‡ç­›é€‰æŠ¥å‘Š - æ€»è§ˆ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**æ€»è®¡**: {len(papers)} ç¯‡è®ºæ–‡

## ğŸ“Š åˆ†ç±»ç»Ÿè®¡

{chr(10).join(category_summary)}

---

## ğŸ“ è¯¦ç»†æŠ¥å‘Š

æ¯ä¸ªåˆ†ç±»çš„è¯¦ç»†æŠ¥å‘Šå·²å•ç‹¬ç”Ÿæˆï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹ã€‚

"""
        
        try:
            with open(overview_filepath, 'w', encoding='utf-8') as f:
                f.write(overview_content)
            logger.info(f"å·²ç”Ÿæˆæ€»è§ˆæŠ¥å‘Š: {overview_filepath}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»è§ˆæŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    def _print_category_summary(self, papers: List[Dict]):
        """
        æ‰“å°åˆ†ç±»ç»Ÿè®¡æ‘˜è¦
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # æŒ‰åˆ†ç±»ç»„ç»‡è®ºæ–‡
        papers_by_category = {}
        for paper in papers:
            tags = paper.get('tags', ['Other'])
            for tag in tags:
                if tag not in papers_by_category:
                    papers_by_category[tag] = []
                papers_by_category[tag].append(paper)
        
        # å®šä¹‰åˆ†ç±»æ˜¾ç¤ºé¡ºåºï¼ˆä¸ generate_markdown_report ä¿æŒä¸€è‡´ï¼‰
        category_order = [
            "KV Cache",
            "LLM Inference",
            "LLM Training (System)",
            "LLM Communication",
            "Video Generation (System)"
        ]
        
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ“Š è®ºæ–‡åˆ†ç±»ç»Ÿè®¡")
        logger.info("=" * 60)
        
        for category in category_order:
            if category in papers_by_category:
                category_papers = papers_by_category[category]
                logger.info(f"\nğŸ“ {category}: {len(category_papers)} ç¯‡")
                logger.info("-" * 60)
                for idx, paper in enumerate(category_papers, 1):
                    # æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾
                    all_tags = paper.get('tags', [])
                    tags_str = ', '.join(all_tags) if len(all_tags) > 1 else ''
                    tags_display = f" [{tags_str}]" if tags_str else ""
                    logger.info(f"  {idx}. {paper['title'][:70]}...{tags_display}")
                    logger.info(f"     arXiv ID: {paper['arxiv_id']}")
        
        logger.info("")
        logger.info("=" * 60)
    
    def run_daily_fetch(self, days_back: int = 1, generate_report: bool = True):
        """
        æ‰§è¡Œæ¯æ—¥æŠ“å–ä»»åŠ¡
        
        Args:
            days_back: å›æº¯å¤©æ•°
            generate_report: æ˜¯å¦ç”Ÿæˆ Markdown æŠ¥å‘Š
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥ arXiv è®ºæ–‡æŠ“å–ä»»åŠ¡")
        logger.info("=" * 60)
        
        try:
            # è·å–è®ºæ–‡
            papers = self.fetch_daily_papers(days_back=days_back)
            
            if papers:
                # ä¿å­˜ JSON æ•°æ®
                self.save_papers(papers)
                
                # ç”Ÿæˆ Markdown æŠ¥å‘Š
                if generate_report:
                    self.generate_markdown_report(papers)
                
                # è¾“å‡ºåˆ†ç±»ç»Ÿè®¡
                self._print_category_summary(papers)
                
                logger.info(f"ä»»åŠ¡å®Œæˆï¼å…±æ‰¾åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡")
            else:
                logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°çš„åŒ¹é…è®ºæ–‡")
        
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='arXiv è®ºæ–‡æŠ“å–å·¥å…·')
    parser.add_argument(
        '--days',
        type=int,
        default=1,
        help='å›æº¯å¤©æ•°ï¼ˆé»˜è®¤ï¼š1ï¼Œå³ä»Šå¤©ï¼‰'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default=None,
        help='æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤ï¼špaper_data_YYYY.MM.DDï¼Œè‡ªåŠ¨æ·»åŠ æ—¥æœŸï¼‰'
    )
    parser.add_argument(
        '--no-report',
        action='store_true',
        help='ä¸ç”Ÿæˆ Markdown æŠ¥å‘Š'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæŠ“å–å·¥å…·å¹¶æ‰§è¡Œ
    fetcher = ArxivPaperFetcher(data_dir=args.data_dir)
    fetcher.run_daily_fetch(
        days_back=args.days,
        generate_report=not args.no_report
    )


if __name__ == "__main__":
    main()


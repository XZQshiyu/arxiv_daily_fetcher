#!/usr/bin/env python3
"""
arXiv Paper Fetcher Tool
æ¯å¤©è‡ªåŠ¨ä» arXiv è·å–æ›´æ–°çš„è®ºæ–‡ï¼Œå¹¶ç­›é€‰å‡ºåŒ…å«ç‰¹å®šå…³é”®è¯çš„è®ºæ–‡
"""

import arxiv
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Set
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
import sys

# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆWindows å…¼å®¹ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# è·å–é¡¹ç›®æ ¹ç›®å½•ç”¨äºæ—¥å¿—æ–‡ä»¶
project_root = Path(__file__).parent.parent
log_file = project_root / 'arxiv_fetcher.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ArxivPaperFetcher:
    """arXiv è®ºæ–‡æŠ“å–å’Œç­›é€‰å·¥å…·"""
    
    def __init__(self, data_dir: str = None, config_file: str = "config.json"):
        """
        åˆå§‹åŒ–æŠ“å–å·¥å…·
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å¸¦æ—¥æœŸçš„ç›®å½•å
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆsrc çš„çˆ¶ç›®å½•ï¼‰
        project_root = Path(__file__).parent.parent
        
        # åŠ è½½é…ç½®ï¼ˆé…ç½®æ–‡ä»¶è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        if not Path(config_file).is_absolute():
            config_file_path = project_root / config_file
        else:
            config_file_path = Path(config_file)
        self.config = self._load_config(str(config_file_path))
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®å½•ï¼Œä½¿ç”¨å¸¦æ—¥æœŸçš„ç›®å½•åï¼ˆæ”¾åœ¨ result ç›®å½•ä¸‹ï¼‰
        if data_dir is None:
            date_str = datetime.now().strftime("%Y.%m.%d")
            result_dir = project_root / "result"
            data_dir = result_dir / f"paper_data_{date_str}"
        else:
            # å¦‚æœæŒ‡å®šäº†ç›®å½•ï¼Œè½¬æ¢ä¸º Path å¯¹è±¡
            data_dir = Path(data_dir)
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ä»é…ç½®ä¸­è·å–å…³é”®è¯å’Œåˆ†ç±»ä¿¡æ¯
        self.keywords_map = self.config.get('keywords', {})
        self.system_keywords = self.config.get('system_keywords', [])
        self.categories_config = self.config.get('categories', {})
        
        # å·²è®°å½•çš„è®ºæ–‡IDé›†åˆï¼ˆç”¨äºå»é‡ï¼‰
        self.recorded_papers_file = self.data_dir / "recorded_papers.json"
        self.recorded_paper_ids = self._load_recorded_papers()
    
    def _load_config(self, config_file: str) -> Dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
            
        Returns:
            é…ç½®å­—å…¸
        """
        # å¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
        config_path = Path(config_file)
        if not config_path.is_absolute():
            project_root = Path(__file__).parent.parent
            config_path = project_root / config_file
        
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                return config
            except Exception as e:
                logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_default_config()
        else:
            logger.info(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """
        è·å–é»˜è®¤é…ç½®
        
        Returns:
            é»˜è®¤é…ç½®å­—å…¸
        """
        return {
            "keywords": {
                "kv_cache": [
                    "KV cache",
                    "KV Cache",
                    "kv cache",
                    "KVCache"
                ],
                "llm_inference": [
                    "LLM inference",
                    "llm inference",
                    "large language model inference"
                ],
                "llm_training": [
                    "LLM training",
                    "llm training",
                    "large language model training"
                ],
                "llm_communication": [
                    "LLM communication",
                    "llm communication",
                    "communication optimization",
                    "communication efficient",
                    "allreduce",
                    "all-gather",
                    "collective communication",
                    "gradient communication",
                    "communication compression"
                ],
                "video_generation": [
                    "video generation",
                    "video synthesis",
                    "video generation model"
                ]
            },
            "system_keywords": [
                "system",
                "systems",
                "architecture",
                "framework",
                "platform",
                "infrastructure",
                "deployment",
                "serving",
                "serving system",
                "inference system",
                "training system",
                "runtime",
                "engine",
                "pipeline"
            ],
            "categories": {
                "KV Cache": {
                    "keywords": "kv_cache",
                    "requires_system": False
                },
                "LLM Inference": {
                    "keywords": "llm_inference",
                    "requires_system": False
                },
                "LLM Training (System)": {
                    "keywords": "llm_training",
                    "requires_system": True
                },
                "LLM Communication": {
                    "keywords": "llm_communication",
                    "requires_system": False
                },
                "Video Generation (System)": {
                    "keywords": "video_generation",
                    "requires_system": True
                }
            }
        }
    
    def _load_recorded_papers(self) -> Set[str]:
        """åŠ è½½å·²è®°å½•çš„è®ºæ–‡ID"""
        if self.recorded_papers_file.exists():
            try:
                with open(self.recorded_papers_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('paper_ids', []))
            except Exception as e:
                logger.warning(f"åŠ è½½å·²è®°å½•è®ºæ–‡å¤±è´¥: {e}")
                return set()
        return set()
    
    def _save_recorded_papers(self):
        """ä¿å­˜å·²è®°å½•çš„è®ºæ–‡ID"""
        try:
            data = {
                'paper_ids': list(self.recorded_paper_ids),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.recorded_papers_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜å·²è®°å½•è®ºæ–‡å¤±è´¥: {e}")
    
    def _check_keywords(self, paper: arxiv.Result) -> bool:
        """
        æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ…å«å…³é”®è¯ï¼ˆåŸºäºé…ç½®æ–‡ä»¶ï¼‰
        
        Args:
            paper: arxiv è®ºæ–‡å¯¹è±¡
            
        Returns:
            å¦‚æœåŒ…å«å…³é”®è¯è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        text_to_check = f"{paper.title} {paper.summary}".lower()
        
        # éå†æ‰€æœ‰åˆ†ç±»é…ç½®
        for category_name, category_config in self.categories_config.items():
            keyword_group = category_config.get('keywords')
            requires_system = category_config.get('requires_system', False)
            
            # è·å–è¯¥åˆ†ç±»çš„å…³é”®è¯åˆ—è¡¨
            keywords = self.keywords_map.get(keyword_group, [])
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥åˆ†ç±»çš„å…³é”®è¯
            if any(kw.lower() in text_to_check for kw in keywords):
                # å¦‚æœéœ€è¦ system é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å« system å…³é”®è¯
                if requires_system:
                    if any(sys_kw.lower() in text_to_check for sys_kw in self.system_keywords):
                        return True
                else:
                    return True
        
        return False
    
    def _categorize_paper(self, paper: arxiv.Result) -> List[str]:
        """
        å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»ï¼ˆåŸºäºé…ç½®æ–‡ä»¶ï¼‰
        
        Args:
            paper: arxiv è®ºæ–‡å¯¹è±¡
            
        Returns:
            åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
        """
        categories = []
        text = f"{paper.title} {paper.summary}".lower()
        
        # éå†æ‰€æœ‰åˆ†ç±»é…ç½®
        for category_name, category_config in self.categories_config.items():
            keyword_group = category_config.get('keywords')
            requires_system = category_config.get('requires_system', False)
            
            # è·å–è¯¥åˆ†ç±»çš„å…³é”®è¯åˆ—è¡¨
            keywords = self.keywords_map.get(keyword_group, [])
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥åˆ†ç±»çš„å…³é”®è¯
            if any(kw.lower() in text for kw in keywords):
                # å¦‚æœéœ€è¦ system é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å« system å…³é”®è¯
                if requires_system:
                    if any(sys_kw.lower() in text for sys_kw in self.system_keywords):
                        categories.append(category_name)
                else:
                    categories.append(category_name)
        
        return categories if categories else ["Other"]
    
    def fetch_daily_papers(self, days_back: int = 1, max_results: int = 1000) -> List[Dict]:
        """
        è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
        
        Args:
            days_back: å›æº¯å¤©æ•°ï¼Œé»˜è®¤1å¤©ï¼ˆä»Šå¤©ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹è·å–æœ€è¿‘ {days_back} å¤©çš„ arXiv è®ºæ–‡...")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # æ„å»ºæŸ¥è¯¢ï¼šè·å–æœ€è¿‘æ›´æ–°çš„è®ºæ–‡
        # arXiv ä½¿ç”¨æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD
        date_str = start_date.strftime("%Y%m%d")
        query = f"submittedDate:[{date_str}000000 TO {end_date.strftime('%Y%m%d')}235959]"
        
        logger.info(f"æŸ¥è¯¢æ¡ä»¶: {query}")
        
        # æœç´¢è®ºæ–‡
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        matched_papers = []
        total_checked = 0
        
        try:
            for paper in arxiv.Client().results(search):
                total_checked += 1
                
                # è·³è¿‡å·²è®°å½•çš„è®ºæ–‡
                if paper.entry_id in self.recorded_paper_ids:
                    continue
                
                # æ£€æŸ¥å…³é”®è¯
                if self._check_keywords(paper):
                    categories = self._categorize_paper(paper)
                    paper_info = {
                        'id': paper.entry_id,
                        'arxiv_id': paper.entry_id.split('/')[-1],
                        'title': paper.title,
                        'authors': [author.name for author in paper.authors],
                        'summary': paper.summary,
                        'published': paper.published.isoformat(),
                        'updated': paper.updated.isoformat(),
                        'categories': paper.categories,
                        'tags': categories,
                        'pdf_url': paper.pdf_url,
                        'arxiv_url': paper.entry_id,
                        'found_date': datetime.now().isoformat()
                    }
                    matched_papers.append(paper_info)
                    self.recorded_paper_ids.add(paper.entry_id)
                    
                    logger.info(f"æ‰¾åˆ°åŒ¹é…è®ºæ–‡: {paper.title[:60]}...")
                    logger.info(f"  åˆ†ç±»: {', '.join(categories)}")
                    logger.info(f"  arXiv ID: {paper.entry_id.split('/')[-1]}")
        
        except Exception as e:
            logger.error(f"è·å–è®ºæ–‡æ—¶å‡ºé”™: {e}")
            raise
        
        logger.info(f"å…±æ£€æŸ¥ {total_checked} ç¯‡è®ºæ–‡ï¼Œæ‰¾åˆ° {len(matched_papers)} ç¯‡åŒ¹é…è®ºæ–‡")
        
        return matched_papers
    
    def save_papers(self, papers: List[Dict], filename: str = None):
        """
        ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°æ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨æ—¥æœŸå‘½å
        """
        if not papers:
            logger.info("æ²¡æœ‰æ–°è®ºæ–‡éœ€è¦ä¿å­˜")
            return
        
        if filename is None:
            date_str = datetime.now().strftime("%Y%m%d")
            filename = f"papers_{date_str}.json"
        
        filepath = self.data_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        existing_papers = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_papers = existing_data.get('papers', [])
            except Exception as e:
                logger.warning(f"è¯»å–å·²æœ‰æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆå¹¶å¹¶å»é‡
        all_papers = existing_papers + papers
        seen_ids = set()
        unique_papers = []
        for paper in all_papers:
            if paper['id'] not in seen_ids:
                seen_ids.add(paper['id'])
                unique_papers.append(paper)
        
        # ä¿å­˜
        data = {
            'fetch_date': datetime.now().isoformat(),
            'total_papers': len(unique_papers),
            'papers': unique_papers
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"å·²ä¿å­˜ {len(unique_papers)} ç¯‡è®ºæ–‡åˆ° {filepath}")
        except Exception as e:
            logger.error(f"ä¿å­˜è®ºæ–‡å¤±è´¥: {e}")
            raise
        
        # æ›´æ–°å·²è®°å½•è®ºæ–‡åˆ—è¡¨
        self._save_recorded_papers()
    
    def generate_markdown_report(self, papers: List[Dict], output_file: str = None):
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Šï¼Œä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            output_file: æ€»è§ˆæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        if not papers:
            logger.info("æ²¡æœ‰è®ºæ–‡éœ€è¦ç”ŸæˆæŠ¥å‘Š")
            return
        
        date_str = datetime.now().strftime("%Y%m%d")
        
        # æŒ‰åˆ†ç±»ç»„ç»‡è®ºæ–‡
        papers_by_category = {}
        for paper in papers:
            tags = paper.get('tags', ['Other'])
            for tag in tags:
                if tag not in papers_by_category:
                    papers_by_category[tag] = []
                papers_by_category[tag].append(paper)
        
        # ä»é…ç½®ä¸­è·å–åˆ†ç±»é¡ºåº
        category_order = list(self.categories_config.keys())
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶
        generated_files = []
        for category in category_order:
            if category in papers_by_category:
                category_papers = papers_by_category[category]
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¸åŠ æ—¥æœŸï¼‰
                safe_category_name = category.replace(" ", "_").replace("(", "").replace(")", "").replace("/", "_")
                category_file = f"{safe_category_name}.md"
                category_filepath = self.data_dir / category_file
                
                # ç”Ÿæˆè¯¥åˆ†ç±»çš„ Markdown å†…å®¹
                md_content = f"""# {category} - arXiv è®ºæ–‡æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**åˆ†ç±»**: {category}

**è®ºæ–‡æ•°é‡**: {len(category_papers)} ç¯‡

---

"""
                
                for idx, paper in enumerate(category_papers, 1):
                    authors_str = ", ".join(paper['authors'][:5])
                    if len(paper['authors']) > 5:
                        authors_str += f" et al. ({len(paper['authors'])} authors)"
                    
                    # æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾
                    all_tags = paper.get('tags', [])
                    tags_display = ', '.join(all_tags) if all_tags else 'Other'
                    
                    md_content += f"""## {idx}. {paper['title']}

- **arXiv ID**: [{paper['arxiv_id']}]({paper['arxiv_url']})
- **ä½œè€…**: {authors_str}
- **å‘å¸ƒæ—¶é—´**: {paper['published']}
- **arXivåˆ†ç±»**: {', '.join(paper['categories'])}
- **æ ‡ç­¾**: {tags_display}
- **PDF**: [ä¸‹è½½é“¾æ¥]({paper['pdf_url']})

**æ‘˜è¦**:
{paper['summary']}

---

"""
                
                try:
                    with open(category_filepath, 'w', encoding='utf-8') as f:
                        f.write(md_content)
                    logger.info(f"å·²ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š: {category_filepath}")
                    generated_files.append((category, category_file))
                except Exception as e:
                    logger.error(f"ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šå¤±è´¥ ({category}): {e}")
        
        # ç”Ÿæˆæ€»è§ˆæ–‡ä»¶
        if output_file is None:
            output_file = "arxiv_report.md"
        
        overview_filepath = self.data_dir / output_file
        
        # ç”Ÿæˆæ€»è§ˆå†…å®¹
        category_summary = []
        for category in category_order:
            if category in papers_by_category:
                count = len(papers_by_category[category])
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆä¸åŠ æ—¥æœŸï¼‰
                safe_category_name = category.replace(" ", "_").replace("(", "").replace(")", "").replace("/", "_")
                category_file = f"{safe_category_name}.md"
                category_summary.append(f"- **[{category}]({category_file})**: {count} ç¯‡")
        
        overview_content = f"""# arXiv è®ºæ–‡ç­›é€‰æŠ¥å‘Š - æ€»è§ˆ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

**æ€»è®¡**: {len(papers)} ç¯‡è®ºæ–‡

## ğŸ“Š åˆ†ç±»ç»Ÿè®¡

{chr(10).join(category_summary)}

---

## ğŸ“ è¯¦ç»†æŠ¥å‘Š

æ¯ä¸ªåˆ†ç±»çš„è¯¦ç»†æŠ¥å‘Šå·²å•ç‹¬ç”Ÿæˆï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹ã€‚

"""
        
        try:
            with open(overview_filepath, 'w', encoding='utf-8') as f:
                f.write(overview_content)
            logger.info(f"å·²ç”Ÿæˆæ€»è§ˆæŠ¥å‘Š: {overview_filepath}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»è§ˆæŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    def _print_category_summary(self, papers: List[Dict]):
        """
        æ‰“å°åˆ†ç±»ç»Ÿè®¡æ‘˜è¦
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # æŒ‰åˆ†ç±»ç»„ç»‡è®ºæ–‡
        papers_by_category = {}
        for paper in papers:
            tags = paper.get('tags', ['Other'])
            for tag in tags:
                if tag not in papers_by_category:
                    papers_by_category[tag] = []
                papers_by_category[tag].append(paper)
        
        # ä»é…ç½®ä¸­è·å–åˆ†ç±»æ˜¾ç¤ºé¡ºåº
        category_order = list(self.categories_config.keys())
        
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ“Š è®ºæ–‡åˆ†ç±»ç»Ÿè®¡")
        logger.info("=" * 60)
        
        for category in category_order:
            if category in papers_by_category:
                category_papers = papers_by_category[category]
                logger.info(f"\nğŸ“ {category}: {len(category_papers)} ç¯‡")
                logger.info("-" * 60)
                for idx, paper in enumerate(category_papers, 1):
                    # æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾
                    all_tags = paper.get('tags', [])
                    tags_str = ', '.join(all_tags) if len(all_tags) > 1 else ''
                    tags_display = f" [{tags_str}]" if tags_str else ""
                    logger.info(f"  {idx}. {paper['title'][:70]}...{tags_display}")
                    logger.info(f"     arXiv ID: {paper['arxiv_id']}")
        
        logger.info("")
        logger.info("=" * 60)
    
    def run_daily_fetch(self, days_back: int = 1, generate_report: bool = True):
        """
        æ‰§è¡Œæ¯æ—¥æŠ“å–ä»»åŠ¡
        
        Args:
            days_back: å›æº¯å¤©æ•°
            generate_report: æ˜¯å¦ç”Ÿæˆ Markdown æŠ¥å‘Š
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥ arXiv è®ºæ–‡æŠ“å–ä»»åŠ¡")
        logger.info("=" * 60)
        
        try:
            # è·å–è®ºæ–‡
            papers = self.fetch_daily_papers(days_back=days_back)
            
            if papers:
                # ä¿å­˜ JSON æ•°æ®
                self.save_papers(papers)
                
                # ç”Ÿæˆ Markdown æŠ¥å‘Š
                if generate_report:
                    self.generate_markdown_report(papers)
                
                # è¾“å‡ºåˆ†ç±»ç»Ÿè®¡
                self._print_category_summary(papers)
                
                logger.info(f"ä»»åŠ¡å®Œæˆï¼å…±æ‰¾åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡")
            else:
                logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°çš„åŒ¹é…è®ºæ–‡")
        
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='arXiv è®ºæ–‡æŠ“å–å·¥å…·')
    parser.add_argument(
        '--days',
        type=int,
        default=1,
        help='å›æº¯å¤©æ•°ï¼ˆé»˜è®¤ï¼š1ï¼Œå³ä»Šå¤©ï¼‰'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default=None,
        help='æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤ï¼špaper_data_YYYY.MM.DDï¼Œè‡ªåŠ¨æ·»åŠ æ—¥æœŸï¼‰'
    )
    parser.add_argument(
        '--no-report',
        action='store_true',
        help='ä¸ç”Ÿæˆ Markdown æŠ¥å‘Š'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šconfig.jsonï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæŠ“å–å·¥å…·å¹¶æ‰§è¡Œ
    fetcher = ArxivPaperFetcher(data_dir=args.data_dir, config_file=args.config)
    fetcher.run_daily_fetch(
        days_back=args.days,
        generate_report=not args.no_report
    )


if __name__ == "__main__":
    main()

